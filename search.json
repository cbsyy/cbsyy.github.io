[{"title":"GRPO","url":"/2025/02/26/GRPO/","content":"# 什么是GRPO？\n群体相对策略优化（GRPO）是一种强化学习(RL)算法，用来增强LLM的推理能力。它通过评估彼此相关的响应组来优化模型，可以提高训练效率，用来解决复杂问题和长链思维的推理任务。\n\n## 为什么选择它，而不是采用 近端策略优化（PPO）呢？\nPPO需要单独价值模型来估计每个响应的值，RL管道需要大量计算资源进行迭代评估和优化，将这些方法扩展到llm会造成成本的增加，这需要大量的内存占用和计算要求支持。\n训练价值模型容易出问题，很复杂，特别是那种主观或者细微评价的任务。\n\n## 传统RL存在可扩展性问题\n绝对奖励评估难以适应各种任务，所以很难在推理领域推广。\n\n\n# GRPO是怎么应对这些挑战的呢？\n## 无价值模型评估优化 \n通过比较组内的响应来消除对评论模型的需求，显著减少计算开销。\n## 相对评估\nGRPO不依赖外部评估者，而是使用群体动力学来评估某个响应相对于同一批次中其他响应的表现怎么样。\n## 高效训练\n通过关注基于群体的优势，GRPO简化了奖励估计过程，可以更快、更适用于大型模型。\n\n\n#  GRPO目标函数分解\n","tags":["算法","GRPO"],"categories":["算法"]},{"title":"日常记录技术","url":"/2025/02/19/日常记录技术/","content":"\n\nrm操作来删除旧的容器，然后docker run更新新的启动参数，如果不经常换，就下次start启动就可以了\n\n基于docker部署xinference，先拉取镜像，启动参数-V指定本地模型路径，可以直接读取，不需要重新下载模型\n\ndocker exec -it xinference /bin/bash 可以查看模型是否正确加载\n\ndocker logs -f <xinference> 查看当前运行日志\n\n\nwsl2是个能在windows系统上提供linux环境的子系统，ubuntu是个可以在windows运行linux的系统，然后Anaconda是安装在这个linux环境里的Python环境","tags":["日常记录"],"categories":["日常记录"]},{"title":"学习方法","url":"/2025/02/18/学习/","content":"\n\n## 费曼学习法\n\n主题 交互 反馈 不断修正\n\nAI只是基于文字语言进行符号层面的统计，而人类的经验是基于知识 经验 情感和文化沉淀的复杂过程","tags":["学习方法"],"categories":["学习方法"]},{"title":"谦的问题","url":"/2025/02/13/谦的问题/","content":"\n# 问题：\ncmd和powershell的区别 是什么？\n# 回答：\n简单来说CMD只能支持传统 Windows 命令，无法使用.NET 库中的命令和 Linux 命令。\nPowerShell不仅支持传统 Windows 命令和.NET 库中的命令，还支持部分常用 Linux 命令，功能更丰富。\n\n","tags":["问题"],"categories":["问题"]},{"title":"Dify应用开发","url":"/2025/02/12/Dify应用/","content":"\nollama url\nhttp://localhost:11434\nhttp://host.docker.internal:11434\n\n## Dify下载部署本地\nhttps://blog.csdn.net/He_r_o/article/details/141105083?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522af75d5d326be0c04ed881163630d2e95%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&request_id=af75d5d326be0c04ed881163630d2e95&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~top_positive~default-1-141105083-null-null.142^v101^pc_search_result_base3&utm_term=dify&spm=1018.2226.3001.4187\n\nhttps://blog.csdn.net/u010522887/article/details/141407784\n\n\n\n# 当前任务：\n数据查询---主要看  提供表结构能不能准确生成  查询sql\n识别到表名和字段名\n\n# 实现方式思路：\n用户询问一个问题->llm将问题处理成sql语句->到数据库查询相关内容->返回给llm解析回答\n\n加入agent，但也只是多个表单独查询，并没有真正的多表关联查询。我们自己的做法是在数据库端把表整合为视图，尽可能降低多表关联查询的需求，以降低大模型的难度，获得更稳定的结果\n\n通过循环\n可能要从工程方面入手和llm两方面入手 优化\n主要是数据工程，比如 建立数据集市，LLM只生成集市层面的SQL\n数据集市 是面向业务域 被清洗过的数据\n\n这样做是类似在数据库端把表整合为视图，尽可能降低多表关联查询的需求，以降低大模型的难度，获得更稳定的结果。\n我还做了点小优化 用户传问题 我就把比较难理解的问题提前用kimi帮我生成了sql查询语句 然后相当于存到了知识库，然后到生成sql要去数据库找的时候 就是问题+知识库检索出来的一起转sql查了\n","tags":["Dify","应用"],"categories":["Dify"]},{"title":"llm面试问题","url":"/2025/02/11/llm面试问题/","content":"序号\t问题\t答案\t列1\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n1\t模型微调，怎么判断效果\t判断大型语言模型（LLM）微调的效果需要结合技术指标、任务适配性和实际应用表现综合分析。\n一、技术指标评估\n损失函数（Loss）\n\n训练损失：观察模型在训练集上的损失是否收敛，若持续下降且稳定，说明模型学习了训练数据。\n\n验证损失：在独立验证集上检查损失，若验证损失与训练损失差距过大（如验证损失上升），可能出现过拟合。\n\n传统自然语言处理（NLP）指标\n\n生成任务：使用 BLEU、ROUGE、METEOR 等指标评估生成文本与参考文本的相似性。\n\n分类任务：通过准确率（Accuracy）、F1-score、精确率（Precision）、召回率（Recall）衡量性能。\n\n困惑度（Perplexity）：评估模型对测试数据的预测能力，值越低说明模型越适应任务。\n\n二、任务相关评估\n人工评估（Human Evaluation）\n\n对生成内容进行人工打分，评估维度包括：\n\n相关性：内容是否符合任务需求（如客服问答是否切题）。\n\n流畅性：语言是否自然、符合语法。\n\n事实准确性：生成内容是否包含错误或虚构信息。\n\n多样性：避免重复性回答（例如在创意生成任务中）。\n\n下游任务表现\n\n将微调后的模型嵌入实际业务流，测试端到端效果：\n\n例如，客服场景中统计问题解决率、用户满意度。\n\n代码生成任务中检查代码通过率或编译成功率。\n\n三、领域适应性与泛化能力\n领域内测试\n\n构建领域相关的测试集，验证模型是否掌握专业术语、逻辑和行业规范。\n\n例如：医疗领域模型需准确回答疾病诊断建议，金融领域模型需符合合规性。\n\n跨领域泛化\n\n用未见过的数据（或跨领域数据）测试模型，观察性能是否显著下降，判断是否过拟合。\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n2\t在训练中，损失函数存在震荡但在减少时，如何调参去解决\n\t当训练中损失函数出现震荡但总体在减少时，可以采取以下策略进行调参以解决这一问题：\n调整学习率：如果学习率设置过高，模型可能在训练过程中跳动较大，难以收敛。可以尝试减小初始学习率，例如减小10倍。此外，可以尝试使用学习率调度器，如Step Decay或Exponential Decay，看看是否更稳定。热身（Warmup）策略也是一个好的选择，即在训练初期使用较低的学习率，逐步增加到设定的初始学习率，然后再进行衰减，可以帮助模型在训练初期稳定下来。\n增大批量大小：增大batch size大小可以让梯度估计更加稳定，减少损失波动。不过，这需要更高的显存。可以尝试逐步增大批量大小，观察效果。另一种折中的方法是使用梯度累积（Gradient Accumulation），即在多个小批次上累积梯度，再进行一次更新，这样可以模拟更大的批量大小效果，用时间换空间。\n添加正则化手段：在AdamW中引入更大的权重衰减参数，有助于防止过拟合并平滑训练过程。如果模型结构中没有使用Dropout，可以尝试在适当的层添加Dropout，通常设置为 0.2 到 0.5 之间。\n检查数据集：数据集的质量直接影响到模型的训练效果。如果数据集有问题，如数据不平衡、噪声过多等，可能会导致loss无法收敛。需要对数据集进行清洗和预处理，提高数据质量。\n增加迭代次数：有时候loss不收敛是因为训练次数不够多。可以适当增加迭代次数，给模型更多的时间来调整参数。\n通过上述方法，可以帮助解决损失函数震荡的问题，同时保持损失的减少趋势。\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n3\t怎么用pytorch去写一个能够跑起训练的脚本\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n4\t训练时读取数据，一般会有一个迭代读取的库，你有印象嘛\n\t导入必要的库：导入PyTorch及其相关组件。\n定义模型：创建模型的类，继承自torch.nn.Module。\n定义损失函数和优化器：选择合适的损失函数和优化器。\n准备数据：加载和预处理数据，创建数据加载器。\n训练模型：编写训练循环，进行模型训练。\n评估模型：在验证集或测试集上评估模型性能。\n保存和加载模型：保存训练好的模型，以便将来使用或进一步训练。\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n5\t训练之前对训练数据做预处理，一般这个预处理步骤放在哪里\n\t在面试中，关于大模型训练前的数据预处理，可以简洁回答如下：\n数据预处理关键步骤：\n数据清洗：处理缺失值、异常值和重复数据。\n特征工程：特征选择、构造和编码。\n数据标准化/归一化：统一数据比例，提高模型稳定性。\n数据增强：增加数据多样性，尤其在计算机视觉领域。\n数据分割：分为训练集、验证集和测试集。\n预处理位置：\n数据加载阶段：使用PyTorch的Dataset和DataLoader进行预处理。\n模型训练前：一次性预处理并保存数据，避免重复操作。\n在线预处理：在训练循环中动态调整预处理步骤。\n使用工具：\nPandas：数据清洗和特征工程。\nScikit-learn：数据标准化和归一化。\nAlbumentations：数据增强。\nTorchvision.transforms：图像数据预处理。\n强调预处理对模型性能的重要性，并根据模型需求选择合适的方法。\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n6\t说一下你常用的损失函数和优化器\n\t在面试中，当问到常用的损失函数和优化器时，可以这样简洁回答：\n损失函数：\nMSE：用于回归，计算预测值和真实值差的平方的平均值。\n交叉熵损失：用于分类，衡量预测概率分布与真实分布的差异。\n二元交叉熵损失：用于二分类问题。\nHinge损失：用于SVM和分类问题。\nKL散度损失：用于度量两个概率分布的差异，常用于生成模型。\n优化器：\nSGD：基础优化器，简单但可能收敛慢。\nAdam：自适应学习率，结合动量和RMSprop。\nRMSprop：调整学习率，对非平稳目标有效。\nAdagrad：为不同参数调整学习率，适合稀疏数据。\nAdadelta：Adagrad改进版，解决学习率下降过快问题。\n这些方法各有优缺点，选择时需根据具体问题和数据集特性。\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n7\t有做过目标检测相关的算法嘛\n\t描述你在项目中使用的特定目标检测算法，例如：\nYOLO (You Only Look Once)：一种流行的单阶段目标检测算法，以其速度快和易于实现而闻名。视觉Transformer（ViT）\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n8\ttransformer 的 decoder 结构中有哪些比较经典的算子\n\t\nTransformer 的 Decoder 结构中包含多个经典的算子，这些算子共同构成了 Decoder 的核心功能。以下是其中一些比较经典的算子：\n1. 自注意力机制 (Self-Attention)\n作用：自注意力机制允许解码器在生成每个词时，关注输入序列中的所有位置，从而捕获全局依赖关系。这对于理解句子的语义和语法结构非常重要。\n实现：通过计算查询（Query）、键（Key）和值（Value）之间的注意力权重，解码器可以动态地聚合相关信息。\n2. 编码器-解码器注意力机制 (Encoder-Decoder Attention)\n作用：解码器通过编码器-解码器注意力机制，关注编码器生成的上下文表示，从而将源序列的信息融入到目标序列的生成中。\n实现：解码器的查询（Query）与编码器的键（Key）和值（Value）进行交互，生成注意力输出。\n3. 前馈神经网络 (Feed-Forward Network)\n作用：前馈网络用于对解码器的输出进行非线性变换，进一步提取特征。\n实现：通常由两层线性变换和一个激活函数（如 ReLU）组成。\n4. 残差连接 (Residual Connection)\n作用：残差连接通过将输入直接加到输出上，缓解了深层网络中的梯度消失问题，同时保留了输入信息。\n实现：在每个子层（如自注意力层和前馈网络）的输出上，加上输入，然后通过 LayerNorm 进行归一化。\n5. LayerNorm (层归一化)\n作用：LayerNorm 通过归一化输入，稳定了训练过程，加速了收敛。\n实现：在每个子层的残差连接后，对输入进行归一化处理。\n6. Dropout (随机失活)\n作用：Dropout 通过随机丢弃一部分神经元的输出，防止模型过拟合。\n实现：在每个子层的输出上应用 Dropout。\n7. 线性层 (Linear Layer)\n作用：线性层用于将解码器的输出映射到词汇表大小，生成最终的预测。\n实现：通常在解码器的最后一层，将隐藏状态映射到词汇表大小。\n8. Softmax 函数\n作用：Softmax 函数将线性层的输出转换为概率分布，每个词汇对应一个概率值。\n实现：对线性层的输出应用 Softmax 函数，生成每个词的概率分布。\n9. 掩码机制 (Masking)\n作用：掩码机制用于防止解码器在生成当前词时看到未来的信息，确保生成过程的自回归性。\n实现：在自注意力层中，通过掩码矩阵将未来位置的注意力权重设置为负无穷大，从而在计算中忽略这些位置。\n这些算子共同构成了 Transformer Decoder 的核心结构，使其能够高效地处理序列到序列的任务，如机器翻译、文本生成等。\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n9\t掩码多头自注意力机制的掩码是什么意思\n\t在 Transformer 的 Decoder 结构中，掩码多头自注意力机制（Masked Multi-Head Attention）中的掩码（Mask）主要用于防止模型在生成当前词时“偷看”未来的词，确保生成过程的因果性。以下是掩码的具体含义和作用：\n1. 掩码的作用\n防止未来信息泄露：在解码器中，掩码确保模型在生成每个词时，只能关注到当前词及之前的词，而不能看到未来的词。这通过将未来位置的注意力权重设置为负无穷大来实现，使得这些位置在经过 softmax 操作后权重接近于 0，从而被忽略。\n处理变长序列：掩码还可以用于屏蔽填充（padding）部分，确保模型只关注真实的词元，而不是填充的无意义部分\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n10\t有用docker做过哪些操作\n\t在大模型开发中，使用Docker进行操作时，通常会涉及以下几种常见任务：\n1. 拉取镜像\n从Docker Hub拉取镜像：使用docker pull命令拉取官方或第三方提供的镜像。例如，拉取最新的Ubuntu镜像可以使用docker pull ubuntu。\n从私有仓库拉取镜像：如果使用私有仓库，需要先使用docker login登录到私有仓库，然后通过docker pull拉取镜像。\n2. 构建镜像\n通过Dockerfile构建镜像：编写Dockerfile定义镜像的构建步骤，然后使用docker build -t name:tag命令构建镜像。例如：\n\nFROM ubuntu:20.04\nRUN apt-get update && apt-get install -y python3\nCOPY . /app\nWORKDIR /app\nCMD [\"python3\", \"script.py\"]\n然后运行docker build -t my_image:latest .来构建镜像。\n使用docker commit手动构建镜像：通过修改运行中的容器，然后使用docker commit将其保存为新镜像。不过，这种方式不推荐用于生产环境。\n3. 删除镜像\n删除特定镜像：使用docker rmi命令删除指定的镜像。例如：\n\ndocker rmi my_image:latest\n删除悬空镜像：悬空镜像是未被任何容器使用的镜像层，可以通过docker image prune命令删除。\n删除所有未使用的镜像：使用docker system prune -a命令可以清理所有未使用的镜像、容器和网络。\n这些操作是Docker日常使用中的基础任务，能够帮助开发者高效地管理和优化容器化应用的开发与部署过程。\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n11\t用dockerFile装过哪些包\n\t \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n12\tpython库导入过哪些\n\t\n在Python开发中，导入库是使用其功能的第一步。以下是关于Python库导入的简洁而全面的介绍：\n1. 导入标准库\nPython标准库提供了丰富的模块和工具，可以直接使用import语句导入。例如：\n导入整个模块：\nPython复制\nimport math\n导入特定功能：\nPython复制\nfrom math import sqrt, pi\n2. 导入第三方库\n第三方库需要先通过pip安装，然后在代码中导入。例如：\n安装库：\n\npip install requests\n导入库：\n\nimport requests\n3. 常用库及其导入方法\nNumPy：用于科学计算，提供多维数组对象。\n\npip install numpy\n\nimport numpy as np\nPandas：用于数据分析，提供高效的数据操作工具。\n\npip install pandas\n\nimport pandas as pd\nMatplotlib：用于绘图，创建各种静态、动态和交互式图表。\n\npip install matplotlib\n\nimport matplotlib.pyplot as plt\nScikit-learn：用于机器学习，提供各种分类、回归和聚类算法。\n\npip install scikit-learn\n\nfrom sklearn import datasets\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier\n\n\n\n通过以上方法，你可以高效地导入和使用Python库，从而简化开发过程并提高代码效率。\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n13\tvllm的推理加速是怎么实现的\t\nvLLM的推理加速实现原理主要基于以下几个方面：\n1. PagedAttention算法\nvLLM采用了一种名为PagedAttention的新型注意力算法，这是其推理加速的核心技术之一。PagedAttention算法通过将注意力机制中的键（Key）和值（Value）缓存（KV Cache）分割成更小、更易于管理的块（pages），从而有效管理注意力机制中的内存占用。这种方法减少了vLLM的内存占用，并使其吞吐量超过传统LLM服务方法。\n2. KV Cache优化\n在生成式大模型中，解码器使用Multi-head Self-Attention机制，涉及Key（K）、Query（Q）、Value（V）矩阵运算。后续的词会用到前面词的K和V矩阵，因此在推理过程中，如果能存储前面词的K和V值，就不需要重新计算，从而提升推理速度。vLLM通过PagedAttention技术，将KV Cache划分为多个小块，动态分配这些小块的空间，未被占用的空间可以供其他任务使用，从而避免显存浪费。\n3. 连续批处理（Continuous Batching）\nvLLM支持连续批处理，即在实际推理过程中，一个批次多个句子的输入的token长度可能相差很大，最后生成的模型输出token长度相差也很大。在python朴素推理中，最短的序列会等待最长序列生成完成后一并返回，这意味着本来可以处理更多token的GPU算力在对齐过程中产生了浪费。连续批处理的方式就是在每个句子序列输出结束后马上填充下一个句子的token，做到高效利用算力。\n4. 模型量化\nvLLM支持模型量化技术，如使用bfloat16或float16数据类型，减少模型参数的存储和计算需求，从而提高推理速度。量化技术可以在不显著降低模型性能的情况下，显著减少模型的内存占用和计算成本。\n5. 异步推理引擎\nvLLM提供了异步推理引擎（AsyncLLMEngine），允许模型在处理多个请求时异步执行。这种异步处理方式避免了模型在处理多个请求时的堵塞状态，提高了模型的响应速度和吞吐量。通过异步推理，vLLM可以更高效地利用GPU资源，进一步提升推理性能。\n6. 高效内存管理\nvLLM通过高效的内存管理和资源调度，优化大语言模型的部署和执行。它通过动态分配和管理内存，确保模型在推理过程中能够高效地利用GPU资源，减少内存碎片和显存浪费。\n7. CUDA核心优化\nvLLM针对CUDA核心进行了优化，确保了速度与效率。通过优化CUDA核心，vLLM能够更充分地利用GPU的计算能力，提高了大型语言模型的推理速度。\n8. 分布式推理支持\nvLLM支持分布式推理，能够在多台GPU上并行运行模型，进一步提高推理速度。通过分布式推理，vLLM可以将大型语言模型的推理任务拆分到多个GPU上并行执行，从而大大提高了推理速度。\n通过上述技术，vLLM显著提升了大模型的推理速度和效率，特别适合于需要高并发和高性能的场景。\t简要回答即可\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n14\t什么是大模型幻觉\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n15\t什么是大模型epoch和学习率\n\t在大模型开发中：\nEpoch（训练周期）：\n定义：一个epoch指的是模型完整遍历一次训练数据集的过程。\n作用：确保模型有足够的机会学习到数据集中的每个样本。\n重要性：epoch的数量是训练模型时的一个关键超参数，它影响着模型的训练时间和最终性能。\n学习率（Learning Rate）：\n定义：学习率是优化算法中控制模型参数更新步长大小的超参数。\n作用：决定了模型在损失函数梯度下降过程中每一步的调整幅度。\n重要性：学习率对模型的收敛速度和最终性能至关重要。过高的学习率可能导致训练不稳定或发散，而过低的学习率则可能导致训练过程缓慢，甚至陷入局部最小值。\n在大模型训练中，合理设置epoch和学习率对于模型能否有效学习和泛化至关重要。通常需要通过实验来调整这些超参数，以达到最佳的训练效果。\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n16\tpython的原生库有哪些\n\t\nPython 的标准库非常丰富，包含了许多原生库，它们提供了各种功能以支持大量的编程任务。以下是一些常见的 Python 原生库，按照功能类别进行分类：\n1. 数学和随机操作\nmath：提供数学函数，如三角函数、对数函数、平方根等。\n示例：math.sqrt(x) 计算平方根，math.sin(x) 计算正弦值。\nrandom：生成随机数。\n示例：random.randint(a, b) 生成一个范围内的随机整数，random.shuffle(list) 打乱列表顺序。\n2. 数据压缩与文件处理\ngzip：处理 .gz 压缩文件。\n示例：gzip.open('file.gz', 'rt') 以文本模式读取 .gz 文件。\nzipfile：处理 .zip 文件。\n示例：zipfile.ZipFile('archive.zip', 'w') 创建一个新的 .zip 文件。\npickle：序列化和反序列化 Python 对象。\n示例：pickle.dump(obj, file) 将对象保存到文件，pickle.load(file) 从文件加载对象。\n3. 网络与系统操作\nsocket：网络通信的基础，用于实现 TCP/UDP 套接字。\n示例：socket.socket(socket.AF_INET, socket.SOCK_STREAM) 创建一个 TCP 套接字。\nos：与操作系统交互，如文件操作、进程管理等。\n示例：os.listdir('.') 列出当前目录下的文件，os.path.join('folder', 'file.txt') 拼接路径。\nsys：提供对 Python 解释器的访问，如获取系统参数、退出程序等。\n示例：sys.exit() 退出程序，sys.argv 获取命令行参数。\n4. 线程与进程\nthreading：实现多线程编程。\n示例：threading.Thread(target=func, args=(arg1,)) 创建一个新线程。\nmultiprocessing：实现多进程编程。\n示例：multiprocessing.Process(target=func, args=(arg1,)) 创建一个新进程。\n5. 文本处理\nre：正则表达式操作，用于字符串匹配和替换。\n示例：re.search(pattern, string) 搜索字符串中的模式。\njson：处理 JSON 格式的数据。\n示例：json.loads(json_str) 解析 JSON 字符串，json.dumps(obj) 将 Python 对象转换为 JSON 字符串。\ncsv：处理 CSV 文件。\n示例：csv.reader(file) 读取 CSV 文件，csv.writer(file) 写入 CSV 文件。\n6. 日期与时间\ndatetime：处理日期和时间。\n示例：datetime.datetime.now() 获取当前日期和时间，datetime.timedelta(days=1) 表示时间间隔。\ntime：提供时间相关的函数，如休眠、获取时间戳等。\n示例：time.sleep(seconds) 暂停程序执行，time.time() 获取当前时间戳。\n7. 命令行和外部程序\nargparse：解析命令行参数。\n示例：parser = argparse.ArgumentParser() 创建一个解析器，parser.add_argument() 添加命令行参数。\nsubprocess：运行外部命令。\n示例：subprocess.run(['ls', '-l']) 运行系统命令。\n8. 测试与调试\nunittest：单元测试框架。\n示例：unittest.TestCase 定义测试用例，self.assertEqual(a, b) 断言两个值是否相等。\npdb：内置的调试器，用于调试代码。\n示例：import pdb; pdb.set_trace() 在代码中设置断点。\n9. 其他\ncollections：提供高级的数据结构，如 defaultdict、deque 等。\n示例：collections.defaultdict(int) 创建一个默认值为 0 的字典。\nfunctools：提供高级函数操作，如 lru_cache（缓存装饰器）和 partial（偏函数）。\n示例：@functools.lru_cache 用于缓存函数的结果。\nitertools：提供高效的迭代器工具，如 chain、permutations 等。\n示例：itertools.chain(iter1, iter2) 合并多个迭代器。\n这些原生库提供了丰富多样的功能，能够满足大多数开发需求。掌握这些库的使用，可以显著提高编程效率和代码质量。\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n17\tPPO与DPO的区别\n\tPPO（Proximal Policy Optimization）和DPO（Direct Preference Optimization）都是强化学习中的优化策略，但它们在目标、方法和适用场景上有所不同。\nPPO是一种模型自由的、基于策略梯度的强化学习算法，它通过限制策略更新的幅度来确保训练的稳定性。PPO通常需要明确的奖励信号（Label），在RLHF（基于人类反馈的强化学习）中，通常需要训练一个奖励模型。\nDPO是一种直接优化人类偏好的新方法，提出目的是简化RLHF中的训练流程，避免强化学习算法（如PPO）带来的复杂性。DPO不依赖于传统的奖励信号来优化策略，而是直接通过用户或系统的偏好来调整策略。DPO的工作原理是创建包含正负样本对比的损失函数，通过直接在偏好数据上优化模型来提高性能。它绕过了建模奖励函数这一步，使得训练过程更加直接和高效。\n总结来说，PPO是一种通用的强化学习策略优化算法，通过限制策略更新的幅度，确保训练的稳定性。而DPO通过直接利用人类的偏好数据，构建了一个简单而有效的策略优化方法，避免了训练奖励模型的复杂性。选择使用DPO还是PPO，取决于具体的应用场景和需求。\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n18\t大模型的批次是什么意思\n\t在大模型训练中，批次（Batch）指的是一组数据样本的集合，这些样本在一次前向传播和反向传播过程中同时被模型处理。批次大小（Batch Size）是这个集合中样本的数量。使用批次处理数据可以提高计算效率，减少内存消耗，并有助于模型的稳定性和收敛性。\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n19\t解释下模型蒸馏和模型量化\n\t模型蒸馏（Model Distillation）是一种模型压缩和知识迁移的技术，其核心在于将一个大规模、预训练的教师模型（Teacher Model）所蕴含的知识传递给一个规模较小的学生模型（Student Model）。这样做的目标是打造一个在性能上与大型模型相近，但计算资源消耗大幅降低的紧凑模型。\n模型量化（Model Quantization）是深度学习模型优化中的一项关键技术，它通过减少模型参数的位宽来降低模型的存储和计算需求，从而提高模型在各种硬件平台上的运行效率。其核心思想是将模型中的浮点数参数（通常是32位浮点数FP32）转化为低精度的数值表示（如8位整数INT8），这样做可以显著减少模型的存储和计算成本，同时尽量保持模型的性能\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n20\t介绍下langChain的库和方法\n\tLangChain是一个用于构建基于大型语言模型（LLMs）的应用程序的框架。它提供了一系列工具和接口，使开发者能够轻松地组合和部署语言模型。主要特点包括：\n模块化组件：易于使用和扩展。\nLangChain表达式语言（LCEL）：支持声明式地组合链。\n集成：支持多种第三方模型和数据源。\n链和代理：提供现成的链和代理，简化开发流程。\nLangChain旨在简化构建智能应用程序的过程，提高开发效率。\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n21\t模型评测的方法有哪些\n\t就大模型来讲 分为主观评测和客观评测。  客观评测方法有 BLEU、 ROUGE。\nBLEU（Bilingual Evaluation Understudy）：这是一种评估机器翻译文本与人类翻译文本之间相似度的自动化工具。它通过比较机器翻译输出与一组参考翻译（通常是人工翻译）之间的n-gram重叠来工作，并对结果进行平滑处理以避免零概率问题。\nROUGE（Recall-Oriented Understudy for Gisting Evaluation）：ROUGE是一系列评估自动摘要和机器翻译质量的指标，它通过计算机器生成文本与参考摘要之间的重叠来评估内容的覆盖度。ROUGE指标包括ROUGE-N、ROUGE-L和ROUGE-S等不同变体，分别关注n-gram、最长公共子序列和skip-bigram等不同方面的相似度。\nBLEU：侧重准确率，ROUGE侧重召回率（Recall）\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n22\t解释下全参数微调，lora ,QLora 的区别\n\t\n全参数微调、LoRA 和 QLoRA 是大模型微调中的三种方法，它们的主要区别如下：\n1. 全参数微调 (Full Fine-Tuning)\n定义：全参数微调是指对模型的所有参数进行训练，以适应特定任务或数据集。\n优点：\n性能优越：通过对所有参数进行训练，能够充分挖掘模型的潜力，实现更好的性能。\n适用性广：不受限于增量矩阵的秩特性，适用于各种任务和数据集。\n缺点：\n计算资源消耗大：需要训练模型的所有参数，计算资源消耗较大，可能不适合在有限资源环境下进行。\n训练时间长：由于需要训练大量参数，训练时间通常较长，不利于快速迭代和优化。\n适用场景：适用于计算资源充足且追求最佳性能的场景。\n2. LoRA (Low-Rank Adaptation)\n定义：LoRA 是一种轻量级的微调技术，其核心思想是通过低秩矩阵分解来模拟参数的改变量，从而以极小的参数量实现大模型的间接训练。\n优点：\n参数效率高：通过低秩分解引入极少的可训练参数，显著减少内存需求和计算成本。\n训练速度快：低秩近似方法能够快速收敛，提高微调效率，缩短模型上线时间。\n无延迟推理：更新矩阵可以明确地合并到原始冻结权重中，不会引入额外的推理延迟。\n减轻灾难性遗忘：通过保留预训练权重，有效减轻灾难性遗忘问题。\n缺点：\n性能可能下降：由于低秩近似可能带来的信息损失，LoRA 在微调过程中可能会导致模型性能下降，特别是在处理复杂任务时。\n适用性限制：主要适用于具有低秩特性的增量矩阵，对于不具备这种特性的任务或数据集，LoRA 可能无法发挥优势。\n适用场景：适用于计算资源有限或需要快速上线的场景。\n3. QLoRA (Quantized Low-Rank Adaptation)\n定义：QLoRA 是 LoRA 的量化版本，结合了 LoRA 方法与深度量化技术，进一步减少训练所需的显存和计算资源。\n优点：\n显存需求低：通过将模型参数量化为 4 位精度（NF4），显著减少模型存储需求，同时保持模型精度的最小损失。\n训练速度快：在训练期间，QLoRA 先以 4-bit 格式加载模型，训练时将数值反量化到 bf16 进行训练，大幅减少了训练所需的显存。\n性能保持：尽管进行了量化，QLoRA 仍然能够保持模型的高性能和准确性。\n缺点：\n量化精度损失：量化过程中可能会引入一定的精度损失，需要设计合适的映射和量化策略以最小化这种损失。\n适用场景：适用于资源极度受限的环境，特别是在需要训练超大规模模型时。\n总结\n全参数微调：适用于计算资源充足且追求最佳性能的场景，但计算成本高，训练时间长。\nLoRA：适用于计算资源有限或需要快速上线的场景，参数效率高，训练速度快，但可能在复杂任务中性能下降。\nQLoRA：适用于资源极度受限的环境，进一步减少了显存需求和计算资源，但需要权衡量化精度损失。\n选择哪种方法取决于具体的应用场景和资源限制。\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n23\tnumpy和pandas库知道是干嘛的嘛\n\t\nNumPy\n定义：NumPy（Numerical Python）是一个开源的Python科学计算库，用于进行高效的数值计算。\n主要功能：\n多维数组操作：NumPy提供了一个强大的N维数组对象ndarray，支持大量的数组操作，如切片、索引、形状变换等。\n数学函数：NumPy包含了大量的数学函数，如三角函数、指数函数、线性代数运算等，可以对数组进行高效的数学运算。\n随机数生成：NumPy提供了生成随机数的函数，如均匀分布、正态分布等，用于模拟和数据分析。\n应用场景：广泛应用于数据分析、科学计算、机器学习等领域，是Python科学计算的基础库。\nPandas\n定义：Pandas是一个开源的Python数据分析库，用于处理和分析结构化数据。\n主要功能：\n数据结构：Pandas提供了两个主要的数据结构DataFrame和Series，DataFrame是一个二维表格，Series是一维数组，方便处理和分析数据。\n数据清洗：Pandas提供了丰富的数据清洗功能，如处理缺失值、异常值、重复值等，可以对数据进行预处理。\n数据转换：Pandas支持数据的转换操作，如分组聚合、透视表、数据重塑等，方便进行数据的转换和分析。\n数据合并：Pandas提供了数据合并的功能，如连接、合并、拼接等，可以将多个数据集合并成一个数据集。\n应用场景：广泛应用于数据分析、数据挖掘、金融分析等领域，是Python数据分析的核心库。\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n24\t微调时，模型遗忘是为什么\n\t在大模型微调过程中，模型遗忘（尤其是灾难性遗忘）是一个常见问题。其主要原因可以总结为以下几点：\n1. 参数更新机制\n在微调过程中，模型通过反向传播更新参数以适应新任务。然而，这种更新可能会破坏模型在预训练阶段学到的通用知识，导致遗忘。例如，新任务的数据分布可能与预训练数据分布差异较大，模型在适应新任务时会过度调整参数，从而覆盖旧知识。\n2. 学习率设置不当\n过高的学习率可能导致模型在微调时对新任务数据过度拟合，而忽视了旧任务的知识。此外，学习率的选择对早期停止策略的效果也很敏感，不当的学习率设置可能会加剧遗忘。\n3. 数据分布差异\n新任务的数据分布与预训练数据分布不一致时，模型容易出现遗忘。当新任务与旧任务的概念距离较大时，遗忘现象更为严重。例如，在医学问答任务上微调的模型，可能会忘记常识问答任务的知识。\n4. 模型容量不足\n如果模型的容量有限，它可能无法同时容纳新旧任务的知识。在这种情况下，模型会优先适应新任务，从而遗忘旧任务的知识。\n5. 缺乏记忆机制\n大模型缺乏类似人脑的选择性遗忘机制，无法像人类一样区分重要和不重要的知识。因此，在学习新任务时，模型可能会无差别地遗忘旧知识。\n6. 优化策略问题\n某些优化策略可能加剧遗忘。例如，完全微调（Full Fine-Tuning）可能会导致模型参数的剧烈变化，从而丢失预训练阶段学到的知识。\n总结\n大模型微调中的遗忘问题主要是由于参数更新机制、数据分布差异、学习率设置不当、模型容量不足以及缺乏记忆机制等因素共同作用的结果。这些因素导致模型在适应新任务时，无法有效保留旧任务的知识，从而出现性能下降和遗忘现象。\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n25\t什么是过拟合和欠拟合\n\t\n在机器学习和大模型开发中，过拟合和欠拟合是两个常见的问题，它们分别描述了模型在训练数据和新数据上的表现差异：\n过拟合（Overfitting）\n定义：过拟合是指模型在训练数据上表现非常好，但在新的、未见过的数据（测试数据）上表现较差的现象。\n原因：\n模型过于复杂：模型的容量（如神经网络的层数和神经元数量）过大，导致模型学习到了训练数据中的噪声和细节，而不是泛化到新数据的能力。\n训练数据不足：训练数据量不足，模型无法从中学习到足够的模式和规律。\n训练时间过长：模型在训练过程中过度拟合训练数据，导致在新数据上表现不佳。\n表现：\n训练集误差低：模型在训练数据上的误差（如损失函数值）非常低。\n测试集误差高：模型在测试数据上的误差显著高于训练数据。\n解决方法：\n增加数据量：通过数据增强或收集更多数据来增加训练数据量。\n降低模型复杂度：减少模型的层数或神经元数量，或使用更简单的模型。\n正则化：使用L1或L2正则化来限制模型的复杂度。\n提前停止：在训练过程中监控验证集的误差，当验证集误差不再下降时停止训练。\n交叉验证：使用交叉验证来评估模型的泛化能力。\n欠拟合（Underfitting）\n定义：欠拟合是指模型在训练数据和新的数据上都表现较差的现象。\n原因：\n模型过于简单：模型的容量不足，无法学习到数据中的复杂模式和规律。\n训练数据不足：训练数据量不足，模型无法从中学习到足够的信息。\n特征选择不当：选择的特征不足以描述数据的模式和规律。\n表现：\n训练集误差高：模型在训练数据上的误差较高。\n测试集误差高：模型在测试数据上的误差也较高。\n解决方法：\n增加模型复杂度：增加模型的层数或神经元数量，或使用更复杂的模型。\n调整超参数：调整模型的超参数，如学习率、正则化参数等。\n增加训练数据量：通过数据增强或收集更多数据来增加训练数据量。\n改进特征工程：选择更有意义的特征或进行特征构造。\n减少正则化：减少或移除正则化项，使模型有更大的自由度来学习数据。\n总结\n过拟合：模型在训练数据上表现很好，但在新数据上表现差，通常是因为模型过于复杂或训练数据不足。\n欠拟合：模型在训练数据和新数据上都表现差，通常是因为模型过于简单或特征选择不当。\n在实际开发中，需要通过调整模型的复杂度、增加数据量、使用正则化和交叉验证等方法来平衡过拟合和欠拟合，以提高模型的泛化能力和性能。\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n26\t为什么选择向量数据库，而不是传统数据库\n\t\n选择向量数据库而不是传统数据库的原因主要包括以下几点：\n1. 处理高维向量数据\n向量数据库：专为存储和检索高维向量数据设计，能够高效处理大规模高维向量数据。\n传统数据库：主要针对结构化数据进行存储和查询，处理高维向量数据时性能较差。\n2. 高效的相似性搜索\n向量数据库：针对向量相似性搜索进行了深度优化，检索速度快、精度高。\n传统数据库：通常使用精确匹配或预定义标准查询，无法高效处理基于语义或上下文的相似性搜索。\n3. 强大的扩展性\n向量数据库：通常采用分布式架构，易于水平扩展，可应对海量数据和高并发查询。\n传统数据库：扩展性相对较弱，难以应对大规模数据和高并发查询。\n4. 丰富的功能特性\n向量数据库：提供完善的向量数据管理、索引构建、查询优化等功能，支持多种相似性搜索算法和距离度量指标。\n传统数据库：功能相对有限，主要支持结构化查询和事务处理。\n5. 灵活的索引选择\n向量数据库：支持多种向量索引算法（如IVF、HNSW、PQ等），可以根据不同的应用场景和数据特点选择最优的索引策略。\n传统数据库：索引类型相对固定，主要针对结构化数据设计。\n6. 应用场景\n向量数据库：广泛应用于RAG（检索增强生成）、推荐系统、语义搜索、图像检索等AI应用。\n传统数据库：主要应用于结构化数据的存储和查询，如企业资源规划（ERP）、客户关系管理（CRM）等。\n7. 成本和可扩展性\n向量数据库：开源解决方案（如Milvus、Weaviate）通常成本较低，适合预算有限的项目。\n传统数据库：通常需要昂贵的硬件设备和软件许可证费用，成本较高。\n综上所述，向量数据库在处理高维向量数据、高效相似性搜索、强大的扩展性和丰富的功能特性等方面具有显著优势，特别适合用于AI应用和大规模数据处理场景。\t主要回答前两项即可\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n27\t介绍下多头注意力机制\t\n多头注意力机制（Multi-Head Attention，MHA）是Transformer架构中的核心组件，通过对传统注意力机制的扩展，显著提升了模型的表达能力和学习效率。以下是其关键流程：\n1.输入变换\n输入序列首先通过三个不同的线性变换层，分别得到查询（Query）、键（Key）和值（Value）矩阵。\n2.分头\n将查询、键和值矩阵分成多个头（即多个子空间），每个头具有不同的线性变换参数。\n3.独立计算注意力\n每个头独立地计算注意力得分，并生成一个注意力加权后的输出。\n4.拼接与线性映射\n将所有头的输出拼接（Concat），并通过一个线性变换层（通常称为输出投影层）得到最终输出。\n优势\n多视角特征提取：通过多个头对数据进行投影，使模型可以关注数据的不同方面。\n提升表达能力：单一注意力机制的容量有限，多头机制可以捕获更多样化的特征。\n并行计算：多头注意力机制可以并行计算，极大提升了效率。\n应用\nTransformer中的应用：多头注意力是Transformer编码器和解码器的核心组件，用于处理输入和输出序列。\nNLP任务：广泛应用于机器翻译、文本摘要、情感分析等任务。\n计算机视觉任务：在图像分类（如Vision Transformer, ViT）和对象检测（如DETR）等任务中也有重要应用。\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n28\tRAG的流程\n\t\nRAG（Retrieval-Augmented Generation）的流程可以总结为以下几个关键步骤：\n知识文档的准备：\n收集和处理各种格式的文档（如Word、TXT、CSV、Excel、PDF等），将其转换为纯文本数据。\n对长篇文档进行切片，分割成多个文本块，以便更高效地处理和检索信息。\n嵌入模型：\n使用预训练的文本嵌入模型（如BERT、GPT等）将文本转换为向量表示，以便在向量空间中进行相似度计算。\n向量数据库：\n将文档的向量表示存储在向量数据库（如FAISS、Milvus等）中，以便进行高效的向量检索。\n检索：\n使用预训练的文本嵌入模型将查询转换为向量表示。\n在向量数据库中检索与查询向量最相似的文档或段落，通常采用双塔模型（Dual-Encoder）进行高效的向量化检索。\n生成：\n将检索到的相关文档与原始查询合并，形成更丰富的上下文信息。\n使用强大的生成模型（如T5或BART）根据输入的上下文信息生成最终的回答或文本。\nRAG技术通过结合检索和生成技术，显著增强了信息检索和生成的质量，广泛应用于问答系统、文档生成等场景。\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n29\tpython的动态类型\n\t动态类型的核心概念\n动态类型：Python是一种动态类型语言，这意味着在编写代码时不需要显式声明变量的类型。变量的类型在运行时自动确定。\n灵活性：这种特性使得编写代码更加灵活，因为开发者可以专注于逻辑而无需过多关注类型声明。\n动态类型的特点\n无需类型声明：在Python中，变量赋值时不需要指定类型，例如：\nPython\n复制\nx = 10          # x 是一个整数\nx = \"hello\"     # x 现在是一个字符串\n类型检查在运行时：Python在运行时检查变量的类型，这可能导致运行时错误，因此需要小心处理类型相关的操作。\n动态绑定：Python支持动态绑定，即可以在运行时动态地添加或修改对象的属性和方法\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n30\t大模型预训练和微调分别起到什么作用\n\t在大模型开发中：\n预训练：在大量通用数据上训练模型，学习语言的基本结构和广泛知识，为特定任务打下基础。\n微调：在特定任务的有限数据上进一步训练模型，调整预训练模型以更好地适应该任务的需求。\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n31\tsql中的inner join 和 left outer join 有什么区别\n\t1. INNER JOIN\nINNER JOIN 只返回两个表中连接条件相匹配的行。如果某行在其中一个表中没有匹配的行，则该行不会包含在结果集中。换句话说，INNER JOIN 返回的是两个表的交集部分。\n2. LEFT OUTER JOIN\nLEFT JOIN（或 LEFT OUTER JOIN）返回左表（LEFT JOIN 关键字后的第一个表）的所有行，即使右表（第二个表）中没有匹配的行。如果左表的某行在右表中没有匹配的行，则结果集中该行的右表部分将为NULL。\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n32\tgroupby 和having 有什么区别\t在SQL中，GROUP BY 和 HAVING 都是与聚合函数一起使用的子句，但它们的作用和用途有所不同：\nGROUP BY\nGROUP BY 子句用于将结果集的行分组，以便可以对每个组执行聚合函数（如 COUNT()、SUM()、AVG()、MAX()、MIN() 等）。GROUP BY 通常与 SELECT 语句一起使用，将数据分成一个或多个组，然后对每个组分别进行聚合计算。\nHAVING\nHAVING 子句的作用是在 GROUP BY 子句对记录进行分组之后，对分组结果进行过滤。HAVING 通常与聚合函数一起使用，它允许你指定过滤分组的条件。HAVING 是 WHERE 子句的扩展，WHERE 子句不能与聚合函数一起使用，而 HAVING 可以。\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n33\tAgent是怎么搭建的，用的是哪个框架？\n\t\n搭建Agent的流程\n定义Agent角色和功能：明确Agent的任务和目标，例如创建一个用户代理和一个助手代理来模拟对话交互。\n选择合适的大语言模型（LLM）：配置使用的语言模型及其参数，如模型名称、API密钥等。\n配置模型参数：设置模型的超参数，如温度、最大生成长度等。\n集成工具和数据源：根据需要集成外部工具和数据源，如搜索引擎、数据库等。\n创建对话流程：定义Agent之间的交互流程，包括消息传递和任务分配。\n测试和优化：测试Agent的功能和性能，根据测试结果进行优化和调整。\n常用框架\nAutoGen：提供多代理对话机制和模块化设计，支持复杂的对话流程和任务协作。\nLangChain：提供链、代理、索引等核心机制，支持复杂的对话流程和任务自动化。\nLlamaIndex：专注于索引和检索，适用于需要快速检索和查询的场景。\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n34\t意图识别是怎么做的\n\thttps://help.aliyun.com/zh/pai/use-cases/llm-based-intent-recognition-solution\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n35\t你在整个项目搭建中做了哪些事\t在面试中，当被问及在整个项目搭建中做了哪些事时，你可以按照以下结构来回答：\n1. 项目规划和需求分析\n目标设定：明确项目的目标和预期成果，包括性能指标和业务目标。\n需求收集：与利益相关者沟通，收集和分析项目需求，确保理解所有功能和非功能需求。\n可行性研究：评估项目的技术可行性、资源需求和潜在风险。\n2. 数据准备和预处理\n数据收集：获取所需的数据集，可能包括公开数据集、内部数据或通过数据采集技术获取的数据。\n数据清洗：处理缺失值、异常值和重复数据，确保数据质量。\n特征工程：选择、构造和转换特征，以提高模型的预测能力。\n数据分割：将数据集分为训练集、验证集和测试集。\n3. 模型选择和设计\n模型调研：研究和评估不同的模型架构，选择最适合项目需求的模型。\n模型设计：设计模型架构，包括选择合适的网络层、激活函数和正则化技术。\n原型开发：快速搭建模型原型，进行初步测试和验证。\n4. 模型训练和调优\n训练环境搭建：设置训练环境，包括硬件配置、软件依赖和开发工具。\n模型训练：使用训练集训练模型，监控训练过程，调整超参数。\n模型调优：使用验证集进行模型调优，包括学习率调整、正则化策略和模型架构调整。\n性能评估：在测试集上评估模型性能，确保模型满足预期的性能指标。\n5. 模型部署和集成\n部署策略：制定模型部署策略，包括部署环境、部署流程和监控机制。\n系统集成：将模型集成到现有的业务系统中，确保模型的可用性和稳定性。\n性能监控：监控模型在生产环境中的性能，包括准确率、响应时间和资源消耗。\n6. 项目管理和协作\n团队协作：与团队成员协作，包括数据科学家、工程师、产品经理和业务分析师。\n进度管理：管理项目进度，确保项目按时完成。\n沟通协调：与利益相关者沟通，协调资源，解决项目中的问题。\n7. 文档和知识共享\n文档编写：编写项目文档，包括设计文档、用户手册和操作手册。\n知识共享：分享项目经验和知识，包括技术报告、博客文章和演讲。\n8. 项目评估和迭代\n项目评估：评估项目成果，包括业务影响、技术成果和团队表现。\n项目迭代：根据项目评估结果，进行项目迭代和优化。\n在回答时，可以根据你的实际经验进行调整和补充，确保回答真实可信。同时，可以强调你在项目中的领导力、创新能力和解决问题的能力。\t回答样板仅供参考\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n36\t如何防止模型在微调时遗忘？\t防止大模型在微调时遗忘的方法有很多，以下是一些常见的策略：\n1. 冻结部分参数\n冻结模型的底层参数（如输入嵌入层等），只更新模型的上层参数。这种方法可以显著减少对旧任务对齐的破坏，同时保留模型对新任务的适应能力。例如，在Biography数据集上，冻结底层参数可将旧任务的准确率从11%提升至44%，而新任务的准确率仍可保持在99%以上。\n2. 使用低秩适应（LoRA）\nLoRA通过在特定层添加低秩矩阵，并只更新这些低秩矩阵，从而减少微调过程中的计算和存储成本。这种方法适用于资源受限的环境，同时也能有效减少遗忘。\n3. 知识蒸馏\n在微调过程中，将新任务学生模型的输出与原始任务教师模型的输出进行对比，从而引导模型保留原有任务的信息。这种方法可以帮助模型在适应新任务的同时，保留旧任务的知识。\n4. 经验重放\n将原始任务的数据和新任务的数据混合，共同训练模型。这种方法可以帮助模型同时学习新旧任务的知识，从而减少遗忘。\n5. 弹性权重巩固（EWC）\n通过对模型参数施加额外约束来保护那些对先前任务重要的权重。EWC的损失函数会惩罚对旧任务重要参数的大幅变化，从而减少遗忘。\n6. 自我蒸馏\n通过模型本身对任务数据进行生成引导，构建自我蒸馏数据集，减少任务数据信息分布与原始模型信息分布的差距。这种方法可以有效缓解微调后的灾难性遗忘。\n7. 适配器（Adapters）\n适配器是添加到现有Transformer架构中的前馈模块，用于减少全连接层之间的参数空间。这种方法可以在不改变原始模型结构的情况下，实现对新任务的快速适应。\n8. 渐进式微调\n使用小的学习率并进行多次微调，逐步调整模型参数，从而减少对旧知识的破坏。\n9. 蒙特卡罗Dropout\n在训练过程中使用Dropout可以帮助模型学习更具泛化性的特征，减少对特定数据的过度拟合。\n10. 多任务学习\n在微调时同时训练多个任务，使模型能够同时学习新旧任务的知识，从而减少遗忘。\n通过结合以上方法，可以根据具体任务和模型的特点，选择合适的策略来有效防止大模型在微调时遗忘旧知识。\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n37\t知道RAG的检索方法有几种吗？\t\n在 Retrieval-Augmented Generation (RAG) 中，检索方法主要可以分为以下几种：\n1. 相似性检索 (Similarity Search)\n定义：通过计算查询向量与存储向量的相似性得分，返回得分最高的记录。\n常见相似性计算方法：\n余弦相似性 (Cosine Similarity)：衡量两个向量之间的夹角余弦值，范围在 [−1,1] 之间，值越大表示越相似。\n欧氏距离 (Euclidean Distance)：衡量两个向量之间的直线距离，距离越小表示越相似。\n曼哈顿距离 (Manhattan Distance)：衡量两个向量在各维度上的绝对差之和，距离越小表示越相似。\n2. 全文检索 (Full-Text Search)\n定义：通过关键词构建倒排索引，在检索时通过关键词进行全文检索，找到对应的记录。\n特点：\n倒排索引：将文档中的每个单词映射到包含该单词的文档列表，从而实现快速检索。\n关键词匹配：通过匹配用户查询中的关键词，找到相关的文档。\n3. 混合检索 (Hybrid Search)\n定义：结合传统的基于关键字的搜索（如 TF-IDF 或 BM25）和现代的语义或向量搜索，将两种方法的结果组合在一起。\n实现方法：\nReciprocal Rank Fusion (RRF)：对不同检索方法的结果进行重新排序，以得到最终的输出结果。\nEnsemble Retriever：将多个检索器（如基于 FAISS 的向量索引和基于 BM25 的检索器）结合起来，利用 RRF 算法进行结果的重排。\n4. HyDE 方案 (HyDE: Hybrid Dense and Sparse Embeddings)\n定义：结合稀疏和稠密嵌入的检索方法，通过双编码器模型生成稀疏和稠密嵌入，从而提升检索效果。\n特点：\n稀疏嵌入：利用传统的关键词匹配方法（如 TF-IDF 或 BM25）生成稀疏嵌入。\n稠密嵌入：利用现代的语义嵌入方法生成稠密嵌入。\n双编码器模型：将稀疏和稠密嵌入结合，生成更全面的检索结果。\n5. 分层索引检索 (Hierarchical Indexing)\n定义：通过构建分层的索引结构，逐步缩小检索范围，提高检索效率。\n特点：\n分层结构：将数据分层存储，先在高层进行粗粒度检索，再在低层进行细粒度检索。\n高效检索：通过分层结构，减少每次检索的计算量，提高检索速度。\n6. 查询重写 (Query Rewriting)\n定义：通过修改和扩展用户查询，生成多个变体查询，以提高检索的召回率。\n实现方法：\n规则重写：使用预定义的规则和模式修改查询。\n机器学习重写：训练模型学习如何根据示例转换查询。\n混合重写：结合规则和机器学习方法。\n7. 重排 (Re-ranking)\n定义：在初步检索结果的基础上，通过额外的模型或算法对结果进行重新排序，以提高检索结果的相关性。\n实现方法：\n交叉编码器 (Cross-Encoder)：使用交叉编码器模型对初步检索结果进行重新排序。\n基于元数据的重排：根据元数据信息对检索结果进行重排。\n总结\nRAG 的检索方法主要包括相似性检索、全文检索、混合检索、HyDE 方案、分层索引检索、查询重写和重排。这些方法各有优缺点，可以根据具体的应用场景和需求选择合适的检索方法，以提高检索效果和生成内容的质量。\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t"},{"title":"deepseek","url":"/2025/02/10/deepseek/","content":"从Deepseek的回答中，也可以看出它的思考模式：\n1、Deepseek先是分析问题的内容，然后去逐一到它的资料库里去搜索关联的对应匹配。\n2、Deepseek会将之前的要求，用到后续的运作中，看似有记忆性。\n3、Deepseek采用穷举法，前面的逐一列举，在下面的回答中，还是重新开始一遍。\n4、Deepseek会有一种类同人类的惰性行为，开始时不作深度思考，在有了新限制之后，会重新调阅资料，再作梳理。\n5、Deepseek的对话设置很具有亲和性，善于承认错误，然后重新开始，不厌其烦，任劳任怨，但容易陷入到固化的思维坑中，只要第一次回答不够理想，下面很难有突破。因为第一次，就是它的天花板。下面的穷举，不可能找出更好的答案。","tags":["deepseek","资料"],"categories":["deepseek"]},{"title":"常用英语单词","url":"/2025/02/10/常用英语单词/","content":"console  控制台","tags":["常用单词"],"categories":["常用单词"]},{"title":"neo4j搭建","url":"/2025/02/10/neo4j搭建/","content":"## 构建neo4j\nhttps://blog.csdn.net/2301_77554343/article/details/135692019?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522259b702cb04ad2c0ae0a173c7ca85210%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&request_id=259b702cb04ad2c0ae0a173c7ca85210&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~top_positive~default-1-135692019-null-null.142^v101^pc_search_result_base3&utm_term=neo4j%E5%AE%89%E8%A3%85%E4%B8%8E%E9%85%8D%E7%BD%AE&spm=1018.2226.3001.4187\n\n","tags":["资料","neo4j"],"categories":["neo4j"]},{"title":"llm面试","url":"/2025/02/08/llm面试/","content":"通过网盘分享的文件：AI大模型\n链接: https://pan.baidu.com/s/1tybsyWsl8TKPD-Vp0JEIsw 提取码: 1fm2 \n--来自百度网盘超级会员v7的分享\n\n通过网盘分享的文件：LLMs大模型拆解版本（92个文件）\n链接: https://pan.baidu.com/s/1FWbwWmAlaSRhTplVitT3fg 提取码: se25 \n--来自百度网盘超级会员v7的分享","tags":["面试"]},{"title":"知识图谱理论","url":"/2025/02/08/知识图谱记载/","content":"利用规范化的语义表示（\nSchema\n&\nOntology\n）\n将碎片化的数据关联和融合\n\n\n当一个人听到或看到一句话的时候，\n他使用自己所有的知识和智能去理解。这不仅包括语法，\n也包括他的词汇知\n识、\n上下文知识，\n更重要的，\n是对相关事物的理解。\n\n知识图谱在\nCV\n领域有广泛的应用，\n将视觉识别出的对象链接到外源的知识图\n谱，\n可用来辅助图像语义关系抽取和视觉语义的深入理解等。\n\n机器与机器之间的相互理解和交流沟通需要设备抽象与设备数据语义化\n终极的万物互联是设备通过规范化的语义进行数据层面的互联\n\n\n知识图谱技术源于互联网，\n最早落地应用的也是搜索引擎、\n智能问\n答和推荐计算等技术领域。\n\n知识图谱通过规范化语义融合多来源数据，\n并能通过图谱推理能力\n支持复杂关联数据的挖掘分析，\n因此在大数据分析领域也有广泛应用。\n\n不论是语言理解和视觉理解，\n外源知识库的引入都可以有力的提升\n语义理解的深度和广度。\n\n知识图谱在医疗、\n金融、\n电商、\n通信等多个垂直领域都有着广泛的\n应用，\n并且每个领域都有其独特的实现和实践方式。\n\n\n\n\n什么是知识？\n人类的自然语言，\n以及创作的绘画和音乐、\n数学语\n言、\n物理模型、\n化学公式等都是人类知识的表示形\n式和传承方式。\n具有获取、\n表示和处理知识的能力\n是人类心智区别于其它物种心智的最本质特征，\n也\n是人脑智能的最本质特征。\n\n##何为 知识表示\n简单而言，\n知识表示（\nKR\n）\n就是用易于计算机处理的方式来描述人脑的知识的方法。\n\nKR\n不是数据格式、\n不等同于数据结构、\n也不是编程语言，\n对于人工智能而言，\n数据与知识的区别在于\nKR\n支持推理。\n\n知识图谱向量表示：\n自然语言：\n为句子中的每个词学一个向量表示\n•\n知识图谱：\n为每个实体和关系学习一个向量表示\n•\n图像视频：为视觉中的每个对象学习一个向量表示\n\n总结（1/2）\n\n知识表示是传统符号人工智能研究的核心，\n知识表示的方法在早期语义网的\n发展过程主要用来为知识图谱的概念建模提供理论基础；\n\n现实的知识图谱项目由于规模化构建的需要，\n常常降低表示的逻辑严格性，\n目前较为常见的知识图谱实践包括RDF图模型和属性图模型；\n\n尽管很多知识图谱并没有应用复杂的知识表示框架，\nSchema工程对于知识图\n谱的构建仍然是基础性和必要性的工作，\n高质量的知识图谱构建通常从\nSchema设计开始。\n\n总结（2/2）\n\n在知识图谱的深度利用中，\n如复杂语义的表达、\n规则引擎的构建、\n推理的实现，\n会对\n更有丰富表达能力的知识表示方法有更多的需求。\n\n图模型是更加接近于人脑认知和自然语言的数据模型，\nRDF作为一种知识图谱表示框\n架的参考标准，\n向上对接OWL等更丰富的语义表示和推理能力，\n向下对接简化后的属\n性图数据库以及图计算引擎，\n仍然是最值得重视的知识图谱表示框架。\n\n知识（图谱）\n的表示学习是符号表示与神经网络相结合比较自然且有前景的方向。知\n识的向量表示有利于刻画那些隐含不明确的知识，\n同时基于神经网络和表示学习实现\n的推理一定程度上可以解决传统符号推理所面临的鲁棒性不高不容易扩展等众多问题。\n\n\n图遍历的查询的物理实现\n主要优化的点：\n1.\n节点的查找，\n关系的查找\n2.\n从节点到关系，\n从关系到节点\n3.\n从关系到关系\n4.\n从节点到属性，\n从关系到属性\n5.\n从关系到关系类型。\n\n\n属性数据的存储处理：\n内联与动态存储\nØ\n图数据库中存在大量属性，\n这些属性的检索与图遍历的计算是分开的，\n这是为了让节点之间的图遍历能不受大量属性数据的影响。\nØ\n节点和关系的存储记录都包含指向它们的第一个属性ID的指针，\n属性记录也是固定大小，\n便于之间通过ID计算获得存储位置。\nØ\n每个属性记录包含多个属性块，\n以及属性链中下一个属性的ID。\nØ\n每个属性记录包含属性类型以及属性索引文件，\n属性索引文件存储属性名称。\nØ\n对于每一个属性值，\n记录包含一个指向动态存储记录的指针（大属性值）或内联值（小属性值）。\n\n\n总结：\n知识图谱存储的选择\nØ\n知识图谱存储方式的选择需要综合考虑性能、动态扩展、实施成本等多方\n面综合因素。\nØ\n区分原生图存储和非原生图存储：\n原生图存储在复杂关联查询和图计算方\n面有性能优势，\n非原生图存储兼容已有工具集通常学习和协调成本会低。\nØ\n区分RDF图存储和属性图存储：\nRDF存储一般支持推理，\n属性图存储通常具\n有更好的图分析性能优势。\nØ\n在大规模处理情况下，\n需要考虑与底层大数据存储引擎和上层图计算引擎\n集成需求。\n\n总结：\n关于图模型与图数据库\nØ\n图模型是更加接近于人脑认知和自然语言的数据模型，\n图数据库是处理复杂的、\n半结构化、\n多维度的、\n紧密关联数据的最好技术。\n我们鼓励在知识图谱项目中\n采用和实践图数据库。\nØ\n图数据库有它弱处，\n假如你的应用场景不包含大量的关联查询，\n对于简单查询，\n传统关系模型和NoSQL数据库目前在性能方面更加有优势。\nØ\nRDF作为一种知识图谱表示框架的参考标准，\n向上对接OWL等更丰富的语义表示\n和推理能力，\n向下对接简化后的属性图模型以及图计算引擎，\n是最值得重视的\n知识图谱表示框架。","tags":["知识图谱"],"categories":["知识图谱"]},{"title":"工作做事","url":"/2025/02/07/工作做事/","content":"---\n### deepseek调研验证 2025/2/5\n春节期间deepseek很火，要把deepseek研究下， 一是适合该模型的指令（或提示词），二是本地化引入deepseekR1\n这个第一点 你用word写个分析手册 ， 把过程都截图下\n怎么加提示词的  加了后效果如何\n\n### 话术训练表 数据处理\n\n话术服务表0116-0201.csv 2025/2/7\n话术训练表0202_0211.xlsx 2025/2/12\n\n这个数据  你从训练内容中  营销员的话术中提炼FABE 这个里面是有分角色的 \n\n话术服务表0212-0218.csv处理至待验证 2025/2/20\n\n话术服务表0114_0218.xlsx  25年2月21日验证后的结果 关联套餐名称等信息后存储到历史文档，新增139条内容。\n\n2/27日\n话术服务表0219-0226.csv（67）条数据进行处理，处理区分客户和小U机器人，解析客户内容，并通过脚本和人工审查处理至待验证。\n25年2/28日\n训练话术0219_0226.xlsx（1173）条数据进行处理，处理区分客户和小U机器人，解析客户内容，并通过脚本和人工审查处理至待验证。\n\n\n### dify研究 2025/2/14\n搭建数据查询应用\n基于agent实现 多个表单独查询\n\n搭建知识问答应用，外挂RAG问答机器人\n\n增加实体识别\n\n增加数据查询通过循环迭代轮次+构建数据集市，从工程化+llm角度对问题进行解析，实现多表查询应用，减少llm负担。\n\n## Fw:Fw: 数据淘查询脚本解析需求（202501全量）2025/2/21\n13522条sql进行处理，\n\n\n附件压缩包是导出的25年1月网优淘全量查询脚本，能否按附件2格式进行解析及归类？\n\n并附上附件3，投喂测试过程，供参考。\n\n\n## 协助领导编写 检察院大模型应用案例 ppt 2025/2/19\n编写了法律知识库与智能检索与检察系统数据安全保障ppt；\n更新生成式AI发展情况至最新版模型发布情况。\n\n\n## 调研春晚机器人并编写调研结果 2025/2/25\n调研 比对宇树科技 and 智元的机器人做的比较OK，调研具体价格，并列出二次开发接口对应文档链接，以及附带github的开源复现地址。\n加入个人具体实现思路以及附带结果总结。\n\n## 数字中国 2025/2/22 \nhttps://www.dcic-china.com/competitions?raceTagFilterId=51\n调研持续关注能搞的比赛。","tags":["工作"]},{"title":"人工智能论文、书","url":"/2025/02/07/人工智能论文、书/","content":"①资料大礼包\n链接：https://pan.baidu.com/s/1jxXY1_rtP1yslJTEhYk-sg \n提取码：8gju \n--来自百度网盘超级会员V1的分享","tags":["book","人工智能论文、书"],"categories":["good book"]},{"title":"llm资料","url":"/2025/02/07/llm资料/","content":"【 太...完整了！】上海交大和腾讯强强联合的机器学习与深度学习课程分享！-人工智能/AI/神经网络 链接：https://pan.baidu.com/s/1Tdo6jCsGoPY4RDGBe_qyjg?pwd=6666\n提取码：6666 \n\n不能错过的Prompt，适用chatgpt高效撰写学术论文(强烈建议收藏)  链接：https://pan.baidu.com/s/1wJHb0ZReiKDJPdrSK7-rGA?pwd=pggh \n链接：https://pan.baidu.com/s/1R0NxwwZUtSGXsFNRU-C78w?pwd=5f8g \n【腾讯文档】50个顶级的ChatGPT学术论文指令\nhttps://docs.qq.com/doc/DUURVUmNCZ3FTckdw \n\n\nhttps://help.aliyun.com/zh/pai/use-cases/best-practices-for-fine-tuning-with-nl2bi?spm=a2c4g.11186623.help-menu-30347.d_3_1_2_9.3fc92dc48zKUQf&scm=20140722.H_2845426._.OR_help-T_cn~zh-V_1\n微调\n\n\nhttps://zh.d2l.ai/chapter_preliminaries/calculus.html\n深度学习（积分）\n\n\nhttps://www.aidoczh.com/ 丰富资源\n\nhttps://milvus.io/docs/zh/how_to_enhance_your_rag.md RAG优化\n\nhttp://wdndev.github.io/llm_interview_note  LLMs 相关知识及面试题\n\n\nhttps://www.langchain.com.cn/docs/introduction/ langchain中文文档\nhttps://python.langchain.com/api_reference/ 英文文档\n\n\n关于记忆，langchain最新有一个LangMem的SDK，也是专门针对Agent的长记忆，有兴趣的也可以看一下：\n\nhttps://blog.langchain.dev/langmem-sdk-launch/\nhttps://langchain-ai.github.io/langmem/?ref=blog.langchain.dev\nhttps://github.com/langchain-ai/langmem\nhttps://www.youtube.com/watch?v=3Yp-hIEcWXk\nhttps://www.youtube.com/watch?v=WW-v5mO2P7w\n","tags":["资料","llm"]},{"title":"python资料","url":"/2025/02/07/python资料/","content":"【Python基础+爬虫系列视频】_图灵Python教育（仅供个人学习研究）\n链接：https://pan.baidu.com/s/1gNgKj6utVGVy6vXGn0J0LQ?pwd=4444 \n提取码：4444\n\n基于python 的机器学习神书，包含TensorFlow 2、scikit-learn、scikit-learn、强化学习和最新的代码实践\n 链接：https://pan.baidu.com/s/1iTJFYIjj-FYJuw4MQAOeuw?pwd=n7we \n提取码：n7we \n--来自百度网盘超级会员V2的分享 ","tags":["资料","python"],"categories":["python"]},{"url":"/2025/02/07/docker资料/","content":"---\ntitle: docker资料\ndate: 2025-02-07 09:51:20\ncategories: docker\ntags:\n- docker\n- 资料\nDocker+Linux使用心得链接：https://pan.baidu.com/s/1OnoBKZuz5PO4J-9f0qbXkg?pwd=3t6j \n提取码：3t6j"},{"title":"强化学习资料","url":"/2025/02/07/强化学习资料/","tags":["资料","强化学习"],"categories":["强化学习"]},{"title":"知识图谱资料","url":"/2025/02/07/知识图谱资料/","content":"膜拜！浙大大牛终于把【知识图谱】讲的如此通透，让人豁然开朗！！\t链接：https://pan.baidu.com/s/1cHwbcpxJ5FlHgcZnEuTnnA\n提取码：t3op\n--来自百度网盘超级会员V1的分享\n\n\n如何从零构建知识图谱？带你快速了解知识图谱的架构与逻辑！ 链接：https://pan.baidu.com/s/1Wsw8W3oJ6XivotAx3Nl5KQ?pwd=ci6v \n提取码：ci6v \n--来自百度网盘超级会员V2的分享  \n\n\n参考链接：\n\n## 大厂技术实现 | 详解知识图谱的构建全流程 @自然语言处理系列\nhttps://cloud.tencent.com/developer/article/1938296\n\n##","tags":["资料","知识图谱"],"categories":["知识图谱"]},{"title":"linux资料","url":"/2025/02/07/linux资料/","content":"后端程序员必备的Linux基础知识链接: https://pan.baidu.com/s/1ZF0AJoz6kiRmtr8PpVawbw 提取码: ygc3\n\nlinux教程资料链接: https://pan.baidu.com/s/1q5IdFllULADxsX4aq5AU4A?pwd=yk72 提取码: yk72\n","tags":["资料","linux"]},{"title":"life","url":"/2024/02/13/life/","content":"摘录：\n> 人生的意义，不在于你拥有多少财富，而在于你拥有多少热情。\n保持就业竞争力所要求的那种“终身学习”，根本不是业余时间看看书、听听讲座的学习强度，而是远超这个，需要你全部时间、全身心地投入，学到筋疲力尽的那种。你要求一个人离开学校以后，比在学校里面还要勤奋、还要努力，对于大多数人来说，这怎么可能！\n\n不是每个人都善于学习，更不是每个人都具有学习的意愿。大多数人只希望生活舒适，不愿意动脑筋，去搞懂那些抽象的公式。而且，要求40、50岁的人跟刚走上社会的年轻人一样拼搏，也不现实。如果终生学习是唯一的就业出路，对于大多数人来说，就是没有出路。\n\n将来，不仅可能出现大量的失业（unemployed），还可能出现人们无法再就业（unemployable），因为他们没用了。低技能的工作，都自动化了；高技能的工作要求多年的学习和艰苦的投入。那些无法就业的人退休还太年轻，从零开始再学习又太老。\n\n如果你不喜欢编程，体会不到代码的乐趣和成就感，只是为了一份好的薪水，就跑来干，那就是很糟糕的选择。想一想如果十年前，你听说国际贸易很兴旺，高考志愿就填了国际贸易，今天会怎样呢？\n\n你应该选择，那些让你产生最大兴趣和热情的职业。因为未来所有行业，低端的、低技能的岗位都会被机器取代，只有技能最强、最有创造性的人不会被淘汰。兴趣，也只有兴趣，才会让你产生不倦的热情，钻研下去，变得更优秀。","tags":["life"],"categories":["life"]},{"title":"生活感悟分享","url":"/2024/01/24/hello-world/","content":"以下内容为摘录文章内容。\n\n为什么你可以不读大学\n作者： 阮一峰\n\n1、\n\n我一直相信，互联网教育是未来的方向。美国三个主要的在线教育网站----Udacity，Coursera，可汗学院----我都经常访问。\n\n2016年四月，Udacity 进入中国，推出了中文版“优达学城”，一下子引起了我的兴趣。因为它干了一件没有先例的事情：颁发网络文凭。它办了一个网上的“硅谷大学”，自己发文凭，名称是“纳米学位”。\n\n“纳米学位（Nanodegree）是优达学城此前与 Google、Facebook、亚马逊等互联网公司联合推出的学历认证项目。学员在线学习，所有项目考核合格之后即可获得纳米学位。”\n\n现在总共有12种纳米学位，包括机器学习、无人驾驶车开发、VR 开发这样非常前沿的领域。\n\n官网这样介绍：\n\n“我们没有严格意义上的录取流程，对报名者唯一的要求是学习该纳米学位项目所必须的先修知识和技能。纳米学位项目采取自主学习模式，你可以按照你喜欢的速度完成项目。”\n\n该公司宣称，国内的许多互联网公司（比如滴滴出行、优酷土豆、京东、新浪）已经认可了纳米学位。\n\n我忍不住想，会不会以后找工作，大家手里拿的不是大学文凭，而是网站颁发的文凭？如果雇主认可网络文凭，我们是否还需要大学文凭？\n\n下面是我的一些思考。\n\n2、\n\n当代的大学起源于欧洲修道院的模式。学生要经过多年的苦修，经过考核，才能毕业。如果想成为高级僧侣，就必须再多熬几年。另外，还有导师作为监督人，防止你学到歪门邪说。\n\n这种模式的两大弊端，演变到今天，已经越来越严重了：一个是传授的知识老化，另一个是极其浪费学生的时间。\n\n3、\n\n什么知识才是有用的知识？\n\n农业社会，上一代人的知识可以一成不变地用在下一代。而在信息社会，前几年的知识，再过几年就不能用了。\n\n举例来说，眼下就业前景最好的行业，我觉得有两个：区块链和VR。它们在五年前都是不存在的，那时就业最好的是苹果iOS系统的应用开发，可是再往前推五年，它也是不存在的。伴随着它们的是，很多旧工作岗位的消失，比如塞班、黑莓、Windows Phone的开发。\n\n这种情况下，大学应该教什么，我们根本不知道。学生毕业后的行业，现在根本还没有出现。因此，大学只能重点教基础类课程，而且各个方向都必须教到，因为不知道学生将来会用到哪个方向的东西。这样就会耗费大量的时间，学习专业的各种基础知识，其中许多对人生来说是没用的。学生常常感叹，考试一结束，有些课程这辈子再没有用到的机会了。\n\n更糟糕的是，学生的培养计划，都是一些二三十年前毕业、然后一直待在大学里、与社会生产实践脱节的人制定的。他们的知识和思维早已过时了。这样的人指定你应该学习的知识，很可能在你学的时候就已经过时了。\n\n4、\n\n退一步说，就算你在大学里能学到了真正的知识，那也不应该在那里待四年。如果只学最需要学习的东西，一年就够了。\n\n四年时间足以让一个人在任何领域成为资深业者，甚至专家。可是我们的大学生呢，经过本科四年，不要说领域专家，甚至能力强的学生都寥寥无几。我们的大学制度用了四年时间，培养出了大量一无所长的、迷茫困惑的、市场滞销的年轻人。\n\n18岁是人生最有热情和精力投入一项事业的时候，但是，大学将你一连四年关在教室和图书馆里，把考试和绩点伪装成你奋斗的目标，人为将你与真实世界隔离，引导你去关注那些对未来人生毫不重要的事情。经过这样四年的歧途，等你真正走上社会、要跟全世界竞争的时候，你的竞争力不是变强了，而是变弱了。换句话说，四年制大学很可能是削弱你，而不是让你变得更强。\n\n2014年诺贝尔物理学奖得主中村修二，就曾经写过一篇长文，名字就叫《东亚教育浪费了太多的生命》。\n\n世界著名程序员 Jamie Zawinski 曾经解释，为什么他只读了一个学期的大学就退学。\n\n“进了大学以后，每天8点就要起床，开始训练记忆力。有一门课我早就会了，想申请免修。教务长说不行，你必须上，这是政策。见鬼，我为什么要自己付钱，来这种地方。我就退学了，从来没后悔过。”\n\n我们时代的很多成功者----乔布斯、比尔盖茨、扎克伯格等等----都是退学生，这绝不是偶然的。不是他们在大学待不下去，而是他们发现，没必要在那个地方待四年。如果他们咬着牙忍受下去，熬到拿到文凭的那一天，苹果公司和微软公司可能都不会有了。\n\n读大学，只是18岁时很多种选择中的一种，不是唯一的选择，更谈不上是最好的选择。校园是一个美丽的地方，但是如果一定要在里面待上四年，那还是算了吧。\n\n5、\n\n德国和瑞士的中学生毕业后，要选择走学术道路还是职业道路。只有不到30%的人，会去读大学，其余的人都接受职业培训，为职业生涯做准备。\n\n我认为，这才是更合理的制度。毕竟大多数人不会从事学术研究，而要靠某种职业谋生。你要知道，大学课程是为学术生涯打基础的，不是为职业生涯设计的。所以，你确定要投入某个职业，合理的选择不是先上大学，然后再找工作，而是一开始就接受职业培训，然后一边工作，一边学习各种对职业生涯有帮助的课程。\n\n理论上，一个人只要接受了中等教育，就可以进社会了。大学的本意是为那些走学术生涯的人开设的，后来慢慢变味了，以至于现在社会上居然有一种说法，”大学是素质教育“。不是这样的，任何时间地点，你都有机会提高素质。\n\n另外，从经济角度看，如果你不想走学术道路，却去读大学，将不利于你的收入。目前，技术工人的薪水，正在不断上升。比他人多几年职业经验，你早早就可以拿到高级技工或工程师的薪水。如果你大学毕业，从零开始就业，你的收入会比同龄人落后几年。如果你还欠了学生贷款，处境就更糟糕了。\n\n6、\n\n注意，我不是说知识无用，而是说知识（尤其是非学术的知识）不一定要通过大学获得，通过互联网一样可以接受高等教育，而且更高效和便宜。\n\n技术已经成为人类社会发展的主导性力量，学习和教育变得比以往更重要、更关键。但是很不幸，我们的学习和教育制度已经完全过时，传授的知识有用的少，没用的多；传授方法仍然依靠灌输和记忆，而不是启发和理解，极其低效，浪费学生的时间，打击学习热情，磨灭对知识的兴趣；对年轻人的成长，正面影响少，负面影响大，而且看不到改变的希望。\n\n以前，人生的选择很少，你不得不去读大学，因为没有其他地方可以接受高等教育。社会还把很多机会与文凭挂钩，先有文凭，然后才能有就业、职称、住房等等。\n\n但是，时代已经变了，文凭正变得越来越不重要。那些与文凭挂钩的东西，正在一项项脱钩。\n\n互联网将教育的自主权，交到了每个人自己手里。上什么课程、什么时间上，都完全由你决定。你可以一边工作，一边利用夜晚和周末，学习网络课程。这样的话，不仅早早就会有收入，而且只学那些对自己最有用、最感兴趣的内容，学习的效率很高。如果发现对学术有兴趣，将来再回大学，攻读更高的学位，也是完全可以的。\n\n等到22岁，别人刚刚开始找工作，还在为归还学贷发愁，你已经有了四年工作经验和一些积蓄，认清了自己的人生道路，开始向事业的高峰冲刺了。\n\n7、\n\n对于那些正在大学里面苦苦努力、不知道方向何在的年轻人，我有一点建议。\n\n大学课程是为了那些不知道学什么的人设计的，千万不要因为自己找不到方向，而被这些课程”画地为牢“限制住。你要主动去接触和学习，那些自己感兴趣的东西。引用一个网友的话，”你要做的就是自主、跨界、终身学习“。","tags":["world"],"categories":["hellow world"]}]